{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9342d-c04c-4cf3-9522-2b319f71b2f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy.ndimage import label\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "MAX_STEPS = 10\n",
    "\n",
    "def generate_percolation_lattice(size, p):\n",
    "    return np.random.choice([0, 1], (size, size), p=[1-p, p]).astype(np.uint8)\n",
    "\n",
    "def check_percolation(lattice):\n",
    "    labeled, _ = label(lattice)\n",
    "    top = set(labeled[0]) - {0}\n",
    "    bottom = set(labeled[-1]) - {0}\n",
    "    left = set(labeled[:,0]) - {0}\n",
    "    right = set(labeled[:,-1]) - {0}\n",
    "    return float(bool(top & bottom) or bool(left & right))\n",
    "\n",
    "def first_coarse_graining(binary_lattice, dim):\n",
    "    \"\"\"Average non-overlapping dim×dim blocks.\"\"\"\n",
    "    t = torch.tensor(binary_lattice, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    patches = F.unfold(t, kernel_size=dim, stride=dim)             # [1, dim*dim, num_patches]\n",
    "    patches = patches.permute(0, 2, 1)                             # [1, num_patches, dim*dim]\n",
    "    coarse_vals = patches.mean(dim=2)                             # [1, num_patches]\n",
    "    H, W = binary_lattice.shape\n",
    "    new_h, new_w = H // dim, W // dim\n",
    "    return coarse_vals.view(1, 1, new_h, new_w).squeeze(0)        # [1, new_h, new_w]\n",
    "\n",
    "class PercolationModel(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.rule = nn.Sequential(\n",
    "            nn.Linear(dim * dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, max_steps=MAX_STEPS):\n",
    "        b, c, H, W = x.shape\n",
    "        for _ in range(max_steps):\n",
    "            if H < self.dim or W < self.dim:\n",
    "                break\n",
    "            patches = F.unfold(x, kernel_size=self.dim, stride=self.dim)  # [b, dim*dim, np]\n",
    "            patches = patches.permute(0, 2, 1).contiguous()               # [b, np, dim*dim]\n",
    "            out = self.rule(patches.view(-1, self.dim*self.dim))          # [b*np, 1]\n",
    "            new_h, new_w = H // self.dim, W // self.dim\n",
    "            x = out.view(b, 1, new_h, new_w)\n",
    "            _, _, H, W = x.shape\n",
    "        return x.squeeze(1).view(b, -1)  # returns shape [b, new_h*new_w] or [b] if fully reduced\n",
    "\n",
    "def prepare_dataset(N, sizes):\n",
    "    data = []\n",
    "    for _ in tqdm(range(int(N/2)), desc=\"Generating data\"):\n",
    "        p = np.random.uniform(0.1, 0.9)\n",
    "        size = np.random.choice(sizes)\n",
    "        L = generate_percolation_lattice(size, p)\n",
    "        data.append((L, check_percolation(L)))\n",
    "    for _ in tqdm(range(int(N/2)), desc=\"Generating fractal data\"):\n",
    "        p = np.random.uniform(0.55,0.65)\n",
    "        size = np.random.choice(sizes)\n",
    "        L = generate_percolation_lattice(size,p)\n",
    "        data.append((L, check_percolation(L)))\n",
    "    return data\n",
    "\n",
    "def train_epoch(model, device, data, batch_size, opt, crit, dim):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    # Process in batches, then split into size groups\n",
    "    for i in tqdm(range(0, len(data), batch_size), desc=\"Training\"):\n",
    "        batch = data[i:i+batch_size]\n",
    "        processed = []\n",
    "        # Apply first_coarse_graining and get sizes\n",
    "        for x, y in batch:\n",
    "            cg_lattice = first_coarse_graining(x, dim)  # [1, H', W']\n",
    "            h, w = cg_lattice.shape[-2], cg_lattice.shape[-1]\n",
    "            processed.append((cg_lattice, y, (h, w)))\n",
    "        \n",
    "        # Group by size\n",
    "        groups = {}\n",
    "        for cg, y, size in processed:\n",
    "            if size not in groups:\n",
    "                groups[size] = []\n",
    "            groups[size].append((cg, y))\n",
    "        \n",
    "        # Process each group\n",
    "        group_loss = 0.0\n",
    "        for size_key, group in groups.items():\n",
    "            lattices = [item[0] for item in group]\n",
    "            labels = [item[1] for item in group]\n",
    "            inputs = torch.stack(lattices).to(device)  # [B, 1, H, W]\n",
    "            targets = torch.tensor(labels, dtype=torch.float32, device=device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            outputs = model(inputs)  # [B, 1]\n",
    "            loss = crit(outputs.view(-1), targets)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            group_loss += loss.item() * len(group)\n",
    "        \n",
    "        total_loss += group_loss\n",
    "    \n",
    "    return total_loss / len(data)\n",
    "\n",
    "def test_systems(model, dim, power, device='cpu',\n",
    "                 num_tests=50, system_size='standard',\n",
    "                 p_range=(0,1), verbose=True):\n",
    "    \"\"\"\n",
    "    For each test:\n",
    "      1) generate a raw DIM^size_power × DIM^size_power lattice\n",
    "      2) compute true percolation label on that raw lattice\n",
    "      3) manually coarse-grain once (patch size = dim)\n",
    "      4) feed the result into model (which will do further recursive steps)\n",
    "    \"\"\"\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # Build a dimension‐dependent mapping from \"dim^k\" strings to exponents\n",
    "    mapping = {\n",
    "        f'{dim}^2': power - 1,\n",
    "        f'{dim}^3': power,\n",
    "        f'{dim}^4': power + 1,\n",
    "        f'{dim}^5': power + 2,\n",
    "        f'{dim}^6': power + 3,\n",
    "        f'{dim}^7': power + 4\n",
    "    }\n",
    "    size_power = mapping[system_size]\n",
    "    L = dim ** size_power\n",
    "\n",
    "    results = []\n",
    "    for _ in tqdm(range(num_tests), desc=f\"Testing {L}×{L}\"):\n",
    "        # 1) Raw lattice + label\n",
    "        p   = np.random.uniform(*p_range)\n",
    "        raw = generate_percolation_lattice(L, p)\n",
    "        lbl = check_percolation(raw)\n",
    "\n",
    "        # 2) Manual first coarse-graining\n",
    "        coarse = first_coarse_graining(raw, dim)   # tensor shape [1, L/dim, L/dim]\n",
    "\n",
    "        # 3) Prepare input for the model\n",
    "        inp = coarse.unsqueeze(0).to(device)       # [1, 1, L/dim, L/dim]\n",
    "\n",
    "        # 4) Get network prediction\n",
    "        with torch.no_grad():\n",
    "            # Let the model do its remaining recursion as usual\n",
    "            # (the `max_steps` is large enough that it will recurse until <dim)\n",
    "            out = model(inp).view(-1).item()\n",
    "\n",
    "        results.append((raw, lbl, out))\n",
    "\n",
    "    # Compute accuracy at 0.5 threshold\n",
    "    acc = sum((pred > 0.5) == lbl for _, lbl, pred in results) / num_tests\n",
    "    pos = [pred for _, lbl, pred in results if lbl == 1]\n",
    "    neg = [pred for _, lbl, pred in results if lbl == 0]\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': acc,\n",
    "        'avg_pred_perc': np.mean(pos) if pos else 0,\n",
    "        'avg_pred_non_perc': np.mean(neg) if neg else 0\n",
    "    }\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nAfter manual first coarse-grain -> NN cascade on {L}×{L}:\")\n",
    "        print(f\" Accuracy        : {acc:.2%}\")\n",
    "        print(f\" Avg pred | Perc     : {metrics['avg_pred_perc']:.3f}\")\n",
    "        print(f\" Avg pred | Non-Perc : {metrics['avg_pred_non_perc']:.3f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def visualize_rule(model, dim, device='cpu', num_samples=1000):\n",
    "    p_values = np.linspace(0, 1, 100)\n",
    "    model.eval()\n",
    "    mean_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for p in p_values:\n",
    "            inputs = torch.full((num_samples, dim*dim), p, \n",
    "                               dtype=torch.float32, device=device)\n",
    "            outputs = model.rule(inputs).cpu().numpy()\n",
    "            mean_outputs.append(outputs.mean())\n",
    "    \n",
    "    # Calculate exact intersections with y=x bisector, ignoring near 0 and 1\n",
    "    mean_outputs = np.array(mean_outputs)\n",
    "    diff = mean_outputs - p_values\n",
    "    crossings = []\n",
    "    for i in range(len(p_values) - 1):\n",
    "        # Only consider points between 0.1 and 0.9\n",
    "        if p_values[i] < 0.1 or p_values[i] > 0.9:\n",
    "            continue\n",
    "        if diff[i] * diff[i+1] <= 0:  # Sign change indicates crossing\n",
    "            # Linear interpolation for precise crossing point\n",
    "            x1, x2 = p_values[i], p_values[i+1]\n",
    "            y1, y2 = diff[i], diff[i+1]\n",
    "            if y1 == y2:\n",
    "                continue  # Avoid division by zero\n",
    "            cross = x1 - y1 * (x2 - x1) / (y2 - y1)\n",
    "            crossings.append(cross)\n",
    "    \n",
    "    # Find critical point (closest approach to bisector in the valid range)\n",
    "    valid_mask = (p_values >= 0.1) & (p_values <= 0.9)\n",
    "    valid_p = p_values[valid_mask]\n",
    "    valid_out = mean_outputs[valid_mask]\n",
    "    abs_diff = np.abs(valid_out - valid_p)\n",
    "    if len(valid_p) > 0:\n",
    "        p_c_model = valid_p[np.argmin(abs_diff)]\n",
    "    else:\n",
    "        p_c_model = p_values[np.argmin(np.abs(mean_outputs - p_values))]\n",
    "    \n",
    "    return p_values, mean_outputs, p_c_model, crossings\n",
    "\n",
    "def run_experiment(dim, power, num_runs=50, device='cpu'):\n",
    "    \"\"\"Run multiple experiments for a given dimension\"\"\"\n",
    "    all_rule_curves = []\n",
    "    all_test_results = defaultdict(list)\n",
    "    all_pc_values = []\n",
    "    \n",
    "    for run_idx in range(num_runs):\n",
    "        print(f\"\\n{'='*40}\")\n",
    "        print(f\"Run {run_idx+1}/{num_runs} for DIM={dim}\")\n",
    "        print(f\"{'='*40}\")\n",
    "        \n",
    "        # Configuration\n",
    "        sizes = [dim**2, dim**3, dim**4]\n",
    "        \n",
    "        # Generate mixed-size training data\n",
    "        train_data = prepare_dataset(10_000, sizes)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = PercolationModel(dim).to(device)\n",
    "        opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        crit = nn.BCELoss()\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(1, 7):\n",
    "            loss = train_epoch(model, device, train_data, 10, opt, crit, dim)\n",
    "            print(f\"Epoch {epoch} — Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Test configurations (using dimension‐dependent labels)\n",
    "        test_configs = [\n",
    "            {'system_size': f'{dim}^2', 'num_tests': 100, 'p_range': (0.55, 0.65)},\n",
    "            {'system_size': f'{dim}^3', 'num_tests': 100, 'p_range': (0.55, 0.65)},\n",
    "            {'system_size': f'{dim}^4', 'num_tests': 100, 'p_range': (0.55, 0.65)},\n",
    "            {'system_size': f'{dim}^5', 'num_tests': 100, 'p_range': (0.55, 0.65)},\n",
    "        ]\n",
    "        \n",
    "        # Run tests\n",
    "        for config in test_configs:\n",
    "            key = f\"{config['system_size']}_{config['p_range'][0]}-{config['p_range'][1]}\"\n",
    "            metrics = test_systems(\n",
    "                model, dim, power, device,\n",
    "                num_tests=config['num_tests'],\n",
    "                system_size=config['system_size'],\n",
    "                p_range=config['p_range'],\n",
    "                verbose=False\n",
    "            )\n",
    "            all_test_results[key].append(metrics)\n",
    "        \n",
    "        # Visualize rule and save curve\n",
    "        p_vals, outputs, p_c_model, crossings = visualize_rule(model, dim, device)  # Now unpacking 4 values\n",
    "        all_rule_curves.append((p_vals, outputs, p_c_model, crossings))  # Storing all 4 values\n",
    "        all_pc_values.append(p_c_model)\n",
    "        \n",
    "        # Clean up to save memory\n",
    "        del model, opt, crit, train_data\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    return all_rule_curves, all_test_results, all_pc_values\n",
    "\n",
    "def save_consolidated_results(results_dict):\n",
    "    with PdfPages(\"consolidated_results.pdf\") as pdf:\n",
    "        for dim, results in results_dict.items():\n",
    "            all_rule_curves, all_test_results, all_pc_values = results\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            # Plot all runs for this dimension\n",
    "            for i, (p_vals, outputs, p_c_model, crossings) in enumerate(all_rule_curves):\n",
    "                # Plot rule curve\n",
    "                plt.plot(p_vals, outputs, alpha=0.5, color='blue')\n",
    "                \n",
    "                # Plot all intersections with bisector (filtered to ignore near 0 and 1)\n",
    "                valid_crossings = [cross for cross in crossings if 0.1 <= cross <= 0.9]\n",
    "                for cross in valid_crossings:\n",
    "                    plt.scatter(cross, cross, color='black', s=30, zorder=3)\n",
    "            \n",
    "            # Plot dotted bisector line y = x\n",
    "            plt.plot([0, 1], [0, 1], 'k--', label=r'$f(p) = p$', linewidth=1.5)\n",
    "            \n",
    "            # Add legend entry for average critical point\n",
    "            avg_pc = np.mean(all_pc_values)\n",
    "            plt.plot([], [], ' ', label=f'Avg Model $p_c$ = {avg_pc:.3f}')\n",
    "            \n",
    "            # Set axis labels with font size\n",
    "            plt.xlabel('Density (p)', fontsize=14)\n",
    "            plt.ylabel(r'$f_{\\theta}(p\\mathbf{1})$', fontsize=14)\n",
    "            \n",
    "            plt.title(f\"Rule Projection - Mixed size - DIM={dim}\", fontsize=18)\n",
    "            plt.legend(fontsize=12)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "            plt.xlim(0, 1)\n",
    "            plt.ylim(0, 1)\n",
    "            pdf.savefig(bbox_inches='tight')\n",
    "            plt.close()\n",
    "    \n",
    "    # Create single UTF-8 text file with all results\n",
    "    with open(\"consolidated_results.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for dim, results in results_dict.items():\n",
    "            all_rule_curves, all_test_results, all_pc_values = results\n",
    "            # Write p_c values\n",
    "            f.write(f\"{'='*40}\\n\")\n",
    "            f.write(f\"Critical Point Analysis (DIM={dim})\\n\")\n",
    "            f.write(f\"{'='*40}\\n\\n\")\n",
    "            f.write(\"p_c values from each run:\\n\")\n",
    "            for i, pc in enumerate(all_pc_values):\n",
    "                f.write(f\"Run {i+1}: {pc:.6f}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nAverage p_c: {np.mean(all_pc_values):.6f}\\n\")\n",
    "            f.write(f\"Standard deviation: {np.std(all_pc_values):.6f}\\n\\n\")\n",
    "            \n",
    "            # Write test results\n",
    "            f.write(f\"{'='*40}\\n\")\n",
    "            f.write(f\"Test Performance Metrics (DIM={dim})\\n\")\n",
    "            f.write(f\"{'='*40}\\n\\n\")\n",
    "            \n",
    "            for config, results_list in all_test_results.items():\n",
    "                # Parse configuration details\n",
    "                parts = config.split('_')\n",
    "                system_size = parts[0]\n",
    "                p_range = parts[1]\n",
    "                \n",
    "                # Extract metrics\n",
    "                accuracies = [r['accuracy'] for r in results_list]\n",
    "                avg_perc = [r['avg_pred_perc'] for r in results_list]\n",
    "                avg_non_perc = [r['avg_pred_non_perc'] for r in results_list]\n",
    "                \n",
    "                # Write header\n",
    "                f.write(f\"System: {system_size}, p-range: {p_range}\\n\")\n",
    "                f.write(\"-\"*50 + \"\\n\")\n",
    "                \n",
    "                # Write detailed run data\n",
    "                f.write(\"Run | Accuracy | Avg Perc | Avg Non-Perc\\n\")\n",
    "                f.write(\"----|----------|----------|------------\\n\")\n",
    "                for i in range(len(results_list)):\n",
    "                    f.write(f\"{i+1:3d} | {accuracies[i]:.4f} | {avg_perc[i]:.4f} | {avg_non_perc[i]:.4f}\\n\")\n",
    "                \n",
    "                # Write summary statistics\n",
    "                f.write(\"\\nSummary Statistics:\\n\")\n",
    "                f.write(f\"Accuracy: {np.mean(accuracies):.4f} ± {np.std(accuracies):.4f}\\n\")\n",
    "                f.write(f\"Avg Perc: {np.mean(avg_perc):.4f} ± {np.std(avg_perc):.4f}\\n\")\n",
    "                f.write(f\"Avg Non-Perc: {np.mean(avg_non_perc):.4f} ± {np.std(avg_non_perc):.4f}\\n\")\n",
    "                f.write(\"=\"*50 + \"\\n\\n\")\n",
    "            f.write(\"\\n\\n\")  # Space between dimensions\n",
    "\n",
    "def main():\n",
    "    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    POWER = 3\n",
    "    \n",
    "    # Collect all results in a dictionary\n",
    "    results_dict = {}\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"STARTING EXPERIMENTS FOR DIM=3\")\n",
    "    print(\"=\"*50)\n",
    "    all_rule_curves_3, all_test_results_3, all_pc_values_3 = run_experiment(\n",
    "        dim=3, power=POWER, num_runs=50, device=DEVICE\n",
    "    )\n",
    "    results_dict[3] = (all_rule_curves_3, all_test_results_3, all_pc_values_3)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"STARTING EXPERIMENTS FOR DIM=4\")\n",
    "    print(\"=\"*50)\n",
    "    all_rule_curves_4, all_test_results_4, all_pc_values_4 = run_experiment(\n",
    "        dim=4, power=POWER, num_runs=50, device=DEVICE\n",
    "    )\n",
    "    results_dict[4] = (all_rule_curves_4, all_test_results_4, all_pc_values_4)\n",
    "    \n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"STARTING EXPERIMENTS FOR DIM=5\")\n",
    "    print(\"=\"*50)\n",
    "    all_rule_curves_5, all_test_results_5, all_pc_values_5 = run_experiment(\n",
    "        dim=5, power=POWER, num_runs=50, device=DEVICE\n",
    "    )\n",
    "    results_dict[5] = (all_rule_curves_5, all_test_results_5, all_pc_values_5)\n",
    "    \n",
    "    # Save all results in consolidated files\n",
    "    save_consolidated_results(results_dict)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
