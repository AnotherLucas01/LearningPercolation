{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9d7ace-2a10-4e91-8660-806f59257f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from scipy.ndimage import label\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "\n",
    "MAX_STEPS = 10\n",
    "\n",
    "def generate_percolation_lattice(size, p):\n",
    "    return np.random.choice([0, 1], (size, size), p=[1-p, p]).astype(np.uint8)\n",
    "\n",
    "def check_percolation(lattice):\n",
    "    labeled, _ = label(lattice)\n",
    "    top = set(labeled[0]) - {0}\n",
    "    bottom = set(labeled[-1]) - {0}\n",
    "    left = set(labeled[:,0]) - {0}\n",
    "    right = set(labeled[:,-1]) - {0}\n",
    "    return float(bool(top & bottom) or bool(left & right))\n",
    "\n",
    "def first_coarse_graining(binary_lattice, dim):\n",
    "    \"\"\"Average non-overlapping dim×dim blocks.\"\"\"\n",
    "    t = torch.tensor(binary_lattice, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "    patches = F.unfold(t, kernel_size=dim, stride=dim)             # [1, dim*dim, num_patches]\n",
    "    patches = patches.permute(0, 2, 1)                             # [1, num_patches, dim*dim]\n",
    "    coarse_vals = patches.mean(dim=2)                             # [1, num_patches]\n",
    "    H, W = binary_lattice.shape\n",
    "    new_h, new_w = H // dim, W // dim\n",
    "    return coarse_vals.view(1, 1, new_h, new_w).squeeze(0)        # [1, new_h, new_w]\n",
    "\n",
    "class PercolationModel(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.rule = nn.Sequential(\n",
    "            nn.Linear(dim * dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, max_steps=MAX_STEPS):\n",
    "        b, c, H, W = x.shape\n",
    "        for _ in range(max_steps):\n",
    "            if H < self.dim or W < self.dim:\n",
    "                break\n",
    "            patches = F.unfold(x, kernel_size=self.dim, stride=self.dim)  # [b, dim*dim, np]\n",
    "            patches = patches.permute(0, 2, 1).contiguous()               # [b, np, dim*dim]\n",
    "            out = self.rule(patches.view(-1, self.dim*self.dim))          # [b*np, 1]\n",
    "            new_h, new_w = H // self.dim, W // self.dim\n",
    "            x = out.view(b, 1, new_h, new_w)\n",
    "            _, _, H, W = x.shape\n",
    "        return x.squeeze(1).view(b, -1)  # returns shape [b, new_h*new_w] or [b] if fully reduced\n",
    "\n",
    "def prepare_dataset(N, sizes):\n",
    "    data = []\n",
    "    for _ in tqdm(range(int(N/2)), desc=\"Generating data\"):\n",
    "        p = np.random.uniform(0.1, 0.9)\n",
    "        size = np.random.choice(sizes)\n",
    "        L = generate_percolation_lattice(size, p)\n",
    "        data.append((L, check_percolation(L)))\n",
    "    for _ in tqdm(range(int(N/2)), desc=\"Generating fractal data\"):\n",
    "        p = np.random.uniform(0.55,0.65)\n",
    "        size = np.random.choice(sizes)\n",
    "        L = generate_percolation_lattice(size,p)\n",
    "        data.append((L, check_percolation(L)))\n",
    "    return data\n",
    "\n",
    "def train_epoch(model, device, data, batch_size, opt, crit):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    # Process in batches, then split into size groups\n",
    "    for i in tqdm(range(0, len(data), batch_size), desc=\"Training\"):\n",
    "        batch = data[i:i+batch_size]\n",
    "        processed = []\n",
    "        # Apply first_coarse_graining and get sizes\n",
    "        for x, y in batch:\n",
    "            cg_lattice = first_coarse_graining(x, DIM)  # [1, H', W']\n",
    "            h, w = cg_lattice.shape[-2], cg_lattice.shape[-1]\n",
    "            processed.append((cg_lattice, y, (h, w)))\n",
    "        \n",
    "        # Group by size\n",
    "        groups = {}\n",
    "        for cg, y, size in processed:\n",
    "            if size not in groups:\n",
    "                groups[size] = []\n",
    "            groups[size].append((cg, y))\n",
    "        \n",
    "        # Process each group\n",
    "        group_loss = 0.0\n",
    "        for size_key, group in groups.items():\n",
    "            lattices = [item[0] for item in group]\n",
    "            labels = [item[1] for item in group]\n",
    "            inputs = torch.stack(lattices).to(device)  # [B, 1, H, W]\n",
    "            targets = torch.tensor(labels, dtype=torch.float32, device=device)\n",
    "            \n",
    "            opt.zero_grad()\n",
    "            outputs = model(inputs)  # [B, 1]\n",
    "            loss = crit(outputs.view(-1), targets)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            group_loss += loss.item() * len(group)\n",
    "        \n",
    "        total_loss += group_loss\n",
    "    \n",
    "    return total_loss / len(data)\n",
    "\n",
    "def test_systems(model, dim, power, device='cpu',\n",
    "                              num_tests=50, system_size='standard',\n",
    "                              p_range=(0,1), verbose=True):\n",
    "    \"\"\"\n",
    "    For each test:\n",
    "      1) generate a raw DIM^size_power × DIM^size_power lattice\n",
    "      2) compute true percolation label on that raw lattice\n",
    "      3) manually coarse-grain once (patch size = dim)\n",
    "      4) feed the result into model (which will do further recursive steps)\n",
    "    \"\"\"\n",
    "    model.to(device).eval()\n",
    "\n",
    "    # Determine the exponent for lattice_size\n",
    "    size_power_dict = {\n",
    "        f'{dim}^2': power-1,\n",
    "        f'{dim}^3': power,\n",
    "        f'{dim}^4': power+1,\n",
    "        f'{dim}^5': power+2,\n",
    "        f'{dim}^6': power+3,\n",
    "        f'{dim}^7': power+4\n",
    "    }\n",
    "    size_power = size_power_dict[system_size]\n",
    "    L = dim ** size_power\n",
    "\n",
    "    results = []\n",
    "    for _ in tqdm(range(num_tests), desc=f\"Testing {L}×{L}\", disable=not verbose):\n",
    "        # 1) Raw lattice + label\n",
    "        p   = np.random.uniform(*p_range)\n",
    "        raw = generate_percolation_lattice(L, p)\n",
    "        lbl = check_percolation(raw)\n",
    "\n",
    "        # 2) Manual first coarse-graining\n",
    "        coarse = first_coarse_graining(raw, dim)   # tensor shape [1, L/dim, L/dim]\n",
    "\n",
    "        # 3) Prepare input for the model\n",
    "        inp = coarse.unsqueeze(0).to(device)       # [1, 1, L/dim, L/dim]\n",
    "\n",
    "        # 4) Get network prediction\n",
    "        with torch.no_grad():\n",
    "            # Let the model do its remaining recursion as usual\n",
    "            # (the `max_steps` is large enough that it will recurse until <dim)\n",
    "            out = model(inp).view(-1).item()\n",
    "\n",
    "        results.append((raw, lbl, out))\n",
    "\n",
    "    # Compute accuracy at 0.5 threshold\n",
    "    acc = sum((pred > 0.5) == lbl for _, lbl, pred in results) / num_tests\n",
    "\n",
    "    if verbose:\n",
    "        pos = [pred for _, lbl, pred in results if lbl==1]\n",
    "        neg = [pred for _, lbl, pred in results if lbl==0]\n",
    "        print(f\"\\nAfter manual first coarse-grain -> NN cascade on {L}×{L}:\")\n",
    "        print(f\" Accuracy        : {acc:.2%}\")\n",
    "        print(f\" Avg pred | Perc     : {np.mean(pos):.3f}\")\n",
    "        print(f\" Avg pred | Non-Perc : {np.mean(neg):.3f}\")\n",
    "\n",
    "    return acc  # Return accuracy instead of full results\n",
    "\n",
    "# ----------------- Main Function -----------------\n",
    "def run_experiment(base, epochs=10, train_samples=2000):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Running experiment for base {base}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    global DIM  # We'll modify the global DIM for this experiment\n",
    "    DIM = base\n",
    "    POWER = 3\n",
    "    SIZES = [base**2, base**3, base**4]  # Training sizes: base^2, base^3, base^4\n",
    "    \n",
    "    # Test sizes - limit to smaller exponents for larger bases\n",
    "    if base <= 4:\n",
    "        TEST_SIZES = [f'{base}^3', f'{base}^4', f'{base}^5', f'{base}^6']  # base^3 to base^6\n",
    "    else:\n",
    "        TEST_SIZES = [f'{base}^3', f'{base}^4', f'{base}^5']  # For base 5, skip base^6\n",
    "    \n",
    "    # Generate training data\n",
    "    train_data = prepare_dataset(train_samples, SIZES)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = PercolationModel(DIM).to(DEVICE)\n",
    "    opt = optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "    crit = nn.BCELoss()\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses = []\n",
    "    gen_errors = {size: [] for size in TEST_SIZES}\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        loss = train_epoch(model, DEVICE, train_data, 10, opt, crit)\n",
    "        train_losses.append(loss)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch} - Loss: {loss:.4f}\")\n",
    "        for size in TEST_SIZES:\n",
    "            # Reduce tests for larger lattices\n",
    "            exp = int(size.split('^')[1])\n",
    "            num_tests = 100 if base**exp <= 1000 else 50  # Fewer tests for larger systems\n",
    "            \n",
    "            acc = test_systems(model, DIM, POWER, DEVICE, num_tests=num_tests, \n",
    "                              system_size=size, p_range=(0.1, 0.9), verbose=False)\n",
    "            error = 1 - acc\n",
    "            gen_errors[size].append(error)\n",
    "            print(f\"Generalization error ({size}): {error:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    epochs_range = range(1, epochs + 1)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Plot training loss\n",
    "    plt.plot(epochs_range, train_losses, 'b-o', label='Training Loss', linewidth=2)\n",
    "    \n",
    "    # Plot generalization errors\n",
    "    colors = ['g', 'r', 'c', 'm', 'y']\n",
    "    for i, size in enumerate(TEST_SIZES):\n",
    "        plt.plot(epochs_range, gen_errors[size], f'{colors[i]}-s', \n",
    "                 label=f'Gen Error ({size})', alpha=0.7)\n",
    "    \n",
    "    plt.xlabel('Epochs', fontsize=16)\n",
    "    plt.ylabel('Loss / Error', fontsize=16)\n",
    "    plt.title(f'Training Loss and Generalization Error (Base {base})', fontsize=20)\n",
    "    plt.legend(fontsize=13)\n",
    "    plt.xticks(epochs_range)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'loss_gen_base{base}.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    return train_losses, gen_errors\n",
    "\n",
    "# ----------------- Run Experiments -----------------\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Run for base 3 (original)\n",
    "run_experiment(base=3, epochs=10, train_samples=1500)\n",
    "\n",
    "# Run for base 4\n",
    "run_experiment(base=4, epochs=10, train_samples=1500)\n",
    "\n",
    "# Run for base 5 (with reduced parameters)\n",
    "run_experiment(base=5, epochs=12, train_samples=1500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
